[{"title":"《Algorithms,Part 1》Programming Assignment 1: Percolation","url":"%2F2019%2F01%2F06%2F%E3%80%8AAlgorithms-Part-1%E3%80%8BProgramming-Assignment-1-Percolation.html","content":"\ncoursera 课程 《Algorithms,Part 1》第一周作业解答 —— 渗透模型。\n\n<!-- more -->\n\n## 问题\n\nProgramming Assignment 1 是一个并查集的应用——渗透模型。\n\n![80D8C615-CF2F-4DDD-9A3B-7460DB13725F.png](https://i.loli.net/2019/01/06/5c31b84b6e708.png)\n\n给定义一个 $n\\times n$ 的矩阵（代表一个系统），黑色代表节点被堵住，白色代表节点已经打开。默认情况下所有节点都被堵住，如果某一个节点与第一行的节点相连（connected），那么它就是 `full` 的。如果最后一行任意一个节点与第一个行任意一个节点相连，那么整个系统就是 `percolation`。\n\n假设每个节点打开的概率是 $p$，求整个系统 percolation 的阀值估计。\n\n![26697FCC-2B43-47B2-A94C-049E697D325A.png](https://i.loli.net/2019/01/06/5c31bf3d3b947.png)\n\n对于这个问题，我们可以使用 `蒙特卡洛模拟(Monte Carlo simulation)`：\n- 所有的节点初始化为关闭（blocked）\n- 重复以下步骤，直到系统实现 percolation\n    - 在所有关闭的节点中随便选择一个\n    - 打开（open）这个节点\n- 此时打开的节点个数/总节点个数就是系统的阀值\n\n假设经过 $T$ 次实验，每次实验的阀值是 $x_t$，则平均值 $\\bar x$ 和方差 $s^2$ 的计算公式如下：\n\n$$ \\bar x=\\frac{x_1+x_2+\\dots+x_T}{T}, s^2=\\frac{(x_1-\\bar x)^2+(x_2-\\bar x)^2+\\dots+(x_T-\\bar x)^2}{T-1} $$\n\n假设 $T$ 足够大，下面给出阀值估计的 $95\\%$ 的置信区间：\n\n$ \\Bigg[ \\bar x-\\frac{1.96s}{\\sqrt{T}}, x+\\frac{1.96s}{\\sqrt{T}} \\Bigg] $\n\n要求实现两个类。Percolation.java 使用给定的 `WeightedQuickUnionUF` 实现以下 API，用于对渗透模型进行操作。\n\n```java\npublic class Percolation {\n   public Percolation(int n)                // create n-by-n grid, with all sites blocked\n   public    void open(int row, int col)    // open site (row, col) if it is not open already\n   public boolean isOpen(int row, int col)  // is site (row, col) open?\n   public boolean isFull(int row, int col)  // is site (row, col) full?\n   public     int numberOfOpenSites()       // number of open sites\n   public boolean percolates()              // does the system percolate?\n\n   public static void main(String[] args)   // test client (optional)\n}\n```\n\nPercolationStas.java 使用设计好的 Percolation 类进行蒙特卡洛模拟，并计算平均值、方差、置信区间等。\n\n```java\npublic class PercolationStats {\n   public PercolationStats(int n, int trials)    // perform trials independent experiments on an n-by-n grid\n   public double mean()                          // sample mean of percolation threshold\n   public double stddev()                        // sample standard deviation of percolation threshold\n   public double confidenceLo()                  // low  endpoint of 95% confidence interval\n   public double confidenceHi()                  // high endpoint of 95% confidence interval\n\n   public static void main(String[] args)        // test client (described below)\n}\n```\n\n\n##  思路\n\nRobert Sedgewick 已经在 Lecture Slides 上提到了一种有效的解决方案，那就是构造虚拟两个节点，以判断整个系统是否是 percolation。\n\n![C1435870-4DB5-453B-B094-E4B51B9F0FFB.png](https://i.loli.net/2019/01/06/5c31ec52650f8.png)\n\n这种方式相当高效，我之前想的一种方法就无奈超时，这样 `isFull()` 和 `percolation()` 方法都是常数时间复杂度，这要比遍历一行节点效率高得多，尤其是第二个类的运行时，遍历的方法大概两分钟才能跑出来结果，而虚拟节点只需要两三秒钟。\n\n不过虚拟节点会出现 `回流` 问题，可以内置两个 WeightedQuickUnionUF 对象，分别用于 `isFull()` 和 `percolation()` 两种方法的记录。\n\n##  实现\n\n[源代码](https://github.com/seriouszyx/Algorithms-solution/tree/master/course/Percolation/src)\n\n好不容易冲到了 99，需要用 `FindBugs` 和 `CheckStyle` 保证代码质量。\n\n有时间把需要注意的地方补充了。\n\n<hr />\n","tags":["Programming Assignment"],"categories":["知识总结"]},{"title":"大数据学习 | 初识 Hadoop","url":"%2F2018%2F12%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0-%E5%88%9D%E8%AF%86Hadoop.html","content":"\n最近想要了解一些前沿技术，不能一门心思眼中只有 web，因为我目前对 Java 语言及其生态相对熟悉，所以在网上搜集了 Hadoop 相关文章，并做了整合。\n\n本篇文章在于对大数据以及 Hadoop 有一个直观的概念，并上手简单体验。\n\n\n<!-- more -->\n\n## Hadoop 基础概念\n\n`Hadoop` 是一个用 Java 实现的开源框架，是一个分布式的解决方案，将大量的信息处理所带来的压力分摊到其他服务器上。\n\n在了解各个名词之前，我们必须掌握一组概念。\n\n### 结构化数据 vs 非结构化数据\n\n`结构化数据`即行数据，存储在数据库里，可以用二维表结构来表达，例如：名字、电话、家庭住址等。\n\n常见的结构化数据库为 mysql、sqlserver。\n\n![zhhihu1.jpg](https://i.loli.net/2018/12/30/5c287655d4f10.jpg)\n\n`非结构化数据库`是指其字段长度可变，并且每个字段的记录又可以由可重复或不可重复的子字段构成的数据库。无法用结构化的数据模型表示，例如：文档、图片、声音、视频等。在大数据时代，对非关系型数据库的需求日益增加，数据库技术相应地进入了“后关系数据库时代”。\n\n非结构化数据库代表为 HBase、mongodb。\n\n![v2-27e5113596ab21aae1d64516ef015100_1200x500.jpg](https://i.loli.net/2018/12/30/5c2876565ece1.jpg)\n\n可以大致归纳，结构化数据是先有结构、再有数据；非结构化数据是先有数据、再有结构。\n\nHadoop 是大数据存储和计算的开山鼻祖，现在大多数开源大数据框架都依赖 Hadoop 或者与它能很好地兼容，下面开始讲述 Hadoop 的相关概念。\n\n### Hadoop 1.0 vs Hadoop 2.0\n\n![Hadoop-1-vs-Hadoop-2-Architecture.png](https://i.loli.net/2018/12/27/5c242519227c9.png)\n\n###  HDFS 和 MapReduce\n\nHadoop 为解决`存储`和`分析`大量数据而生，所以这两部分也是 Hadoop 的狭义说法（广义指 Hadoop 生态）。HDFS 提供了一种安全可靠的分布式文件存储系统，MapReduce 提供了基于批处理模式的数据分析框架。\n\n`HDFS`（Hadoop Distributed File System）的设计本质上是为了大量的数据能横跨很多台机器，但是你看到的是一个文件系统而不是很多个文件系统。就好比访问 `/hdfs/tmp/file1` 的数据，引用的是一个文件路径，但是实际数据可能分布在很多机器上，当然 HDFS 为你管理这些数据，用户并不需要了解它如何管理。\n\n关于 `MapReduce`，这里通过一个具体模型来解释。\n\n考虑如果你要统计一个巨大的文本文件存储在类似 HDFS 上，你想要知道这个文本里各个词的出现频率。你启动了一个 MapReduce 程序。Map 阶段，几百台机器同时读取这个文件的各个部分，分别把各自读到的部分分别统计出词频，产生类似（hello, 12100次），（world，15214次）等等这样的 Pair（我这里把 Map 和 Combine 放在一起说以便简化）；这几百台机器各自都产生了如上的集合，然后又有几百台机器启动 Reduce 处理。Reducer 机器 A 将从 Mapper 机器收到所有以 A 开头的统计结果，机器 B 将收到 B 开头的词汇统计结果（当然实际上不会真的以字母开头做依据，而是用函数产生 Hash 值以避免数据串化。因为类似 X 开头的词肯定比其他要少得多，而你不希望数据处理各个机器的工作量相差悬殊）。然后这些Reducer将再次汇总，（hello，12100）＋（hello，12311）＋（hello，345881）= （hello，370292）。每个 Reducer 都如上处理，你就得到了整个文件的词频结果。\n\n这就是一个简单的 `WordCount` 的例子，Map+Reduce 这种简单模型暴力好用，不过很笨重，关于更高效的解决方法，以后再详细描述。\n\n###  Hadoop 构建模块\n\n下面从底层实现的角度解释 HDFS 和 MapReduce 的一些概念。\n\n`NameNode` 是 Hadoop 守护进程中最重要的一个。NameNode 位于 HDFS 的主端，指导 DataNode 执行底层的 IO 任务。NameNode 的运行消耗大量内存和 IO 资源，所以 NameNode 服务器不会同时是 DataNode 或 TaskTracker。\n\nNameNode 和 `DataNode` 为主/从结构（Master/Slave）。每一个集群上的从节点都会驻留一个 DataNode 守护进程，来执行分布式文件系统的繁重工作，将 HDFS 数据块读取或者写入到本地文件系统的实际文件中。当希望对 HDFS 文件进行读写时，文件被分割为多个块，由NameNode 告知客户端每个数据块驻留在那个 DataNode。客户端直接与 DataNode 守护进程通信，来处理与数据块相对应的本地文件。\n\n`SNN`（Scondary NameNode）是监测 HDFS 集群状态的辅助守护进程。SNN 快照有助于加少停机的时间并降低数据丢失的风险。\n\n`JobTracker` 守护进程是应用程序和 Hadoop 之间的纽带。一旦提交代码到集群上，JobTracker 就会确定执行计划，包括决定处理哪些文件，为不同的任务分配节点以及监控所有任务的运行。如果任务失败，JobTracker 将自动重启任务，但所分配的节点可能会不同，同时受到预定义的重试次数限制。每一个Hadoop集群只有一个JobTracker守护进程，它通常运行在服务器集群的主节点上。\n\nJobTracker 和 `TaskTracker` 也是主/从结构。JobTracker 作为主节点，监测 MapReduce 作业的整个执行过程，同时，TaskTracker 管理各个任务在每个从节点上的执行情况。TaskTracker 的一个职责就是负责持续不断地与 JobTracker 通讯。如果 JobTracker 在指定的时间内没有收到来自 TaskTracker 的心跳，它会假定 TaskTracker 已经崩溃了，进而重新提交相应的任务到集群的其他节点中。\n\n##  尝试使用 Hadoop\n\n`Hadoop 安装`可以直接看官方文档，或是 Google 一些不错的教程，比如 [Hadoop 的安装](https://chu888chu888.gitbooks.io/hadoopstudy/content/Content/4/chapter0401.html)、[Mac 系统安装Hadoop 2.7.3](https://www.jianshu.com/p/de7eb61c983a)。\n\n按照操作配置 Hadoop 并成功运行，访问`localhost:50070` 和 `localhost:8088` 分别显示一下页面。\n\n![90496E3D-A8FB-41CE-9FF0-3B962184AFAE.png](https://i.loli.net/2018/12/27/5c2457210b58d.png)\n\n![1CBC323A-55DC-40AC-B258-3725DD0D4350.png](https://i.loli.net/2018/12/27/5c2457214a2e2.png)\n\n运行`伪分布式`样例：\n\n![31D3E6A6-5864-4C6E-865E-AE576A64E647.png](https://i.loli.net/2018/12/27/5c24700c97c3a.png)\n\n\n### HDFS 目录/文件操作命令\n\nHDFS 是一种文件系统，它可以将一个很大的数据集存储为一个文件，而大多数其他文件系统无力于这一点。Hadoop 也为它提供了一种与 Linux 命令类似的命令行工具，我们可以进行一些简单的操作。\n\nHadoop 的`文件命令`采取的形式为\n\n```shell\nhadoop fs -cmd <args>\n```\n\n其中 cmd 为具体的文件命令，通常与 UNIX 对应的命令名相同，比如：\n\n```shell\nhadoop fs -ls\nhadoop fs -mkdir /user/seriouszyx\nhadoop fs -lsr /\nhadoop fs -rm example.txt\n```\n\n还有一些本地文件系统和 HDFS 交互的命令，也经常使用到。\n\n```shell\nhadoop fs -put example.txt /user/seriouszyx\nhadoop fs -get example.txt\n```\n\n##  Hadoop 构建模块的原理\n\n### MapReduce 如何分而治之\n\nMapReduce 是用来处理大规模数据的一个并行编程框架，采用了对数据“分而治之”的方法。\n\n![40658-2de7c5066daf7ab1.png](https://i.loli.net/2018/12/30/5c28765631bc8.png)\n\nMapReduce 是一个离线计算框架，它将计算分为两个阶段，Map（并行处理输入数据）和 Reduce（对 Map 结果汇总）。其中 Map 和 Reduce 函数提供了两个高层接口，由用户去编程实现。\n\nMap 的一般处理逻辑为：**(k1;v1) ---->map 处理---->[(k2;v2)]**\n\nReduce 函数的一般处理逻辑是：**(k2;[v2])---->reduce 处理---->[(k3;v3)]**\n\n可以看出 map 处理的输出与 reduce 的输入并不完全相同，这是因为输入参数在进入 reduce 前，一般会将相同键 k2 下的所有值 v2 合并到一个集合中处理：**[(k2;v2)]--->(k2;[v2])**，这个过程叫 Combiner。\n\n在经过 Map 和 Reduce 的抽象后，并行结构模型就变成了下面这样：\n\n![40658-df82b7a1775fac75.png](https://i.loli.net/2018/12/30/5c28765664bab.png)\n\n上图中可以发现，中间有一个同步障（Barrier），其作用是等所有的 map 节点处理完后才进入 reduce，并且这个阶段同时进行数据加工整理过程（Aggregation & Shuffle），以便 reduce 节点可以完全基于本节点上的数据计算最终结果。\n\n不过这仍然不是完整的 MapReduce 模型，在上述框架图中，还少了两个步骤 Combiner 和 Partitioner。\n\n![40658-39cc7b851195657c.png](https://i.loli.net/2018/12/30/5c287656a01e5.png)\n\n\n上述图以`词频统计（WordCount）`为例。\n\n**Combiner** 用来对中间结果数据网络传输进行优化，比如 map 处理完输出很多键值对后，某些键值对的键是相同的，Combiner 就会将相同的键合并，比如有两个键值对的键相同（good，1）和（good，2），便可以合成(good,3)。\n\n这样，可以减少需要传输的中间结果数据量，打倒网络数据传输优化，因为 map 传给 reduce 是通过网络来传的。\n\n**Partitioner** 负责对中间结果进行分区处理。比如词频统计，将所有主键相同的键值对传输给同一个 Reduce 节点，以便 Reduce 节点不需要访问其他 Reduce 节点的情况下，一次性对分过来的中间结果进行处理。\n\n### 副本机制\n\n我们再说回 HDFS 诞生的原因，hdfs 由 Google 最先研发，其需求是单独一台计算机所能存储的空间是有限的，而随着计算机存储空间的加大，其价格是呈几何倍的增长。而 hdfs 架构在相对廉价的计算机上，以分布式的方式，这样想要扩大空间之遥增加集群的数量就可以了。\n\n大量相对廉价的计算机，那么说明**宕机**就是一种必然事件，我们需要让数据避免丢失，就只用采取冗余数据存储，而具体的实现的就是`副本机制`。\n\n![](http://hadoop.apache.org/docs/r2.8.3/hadoop-project-dist/hadoop-hdfs/images/hdfsdatanodes.png)\n\nhdfs 主要使用`三副本机制`\n\n- 第一副本：如果上传节点是 DN，则上传该节点；如果上传节点是 NN，则随机选择 DN\n- 第二副本：放置在不同机架的 DN 上\n- 第三副本：放置在与第二副本相同机架的不同 DN 上\n\n除了极大程度地避免宕机所造成的数据损失，副本机制还可以在数据读取时进行数据校验。\n\n### NameNode 在做些什么\n\n在 Hadoop 1.0 时代，Hadoop 两大核心组件 HDFS NameNode 和 JobTracker 都存在着单点问题，其中以 NameNode 最为严重。因为 `NameNode 保存了整个 HDFS 的元数据信息`，一旦 NameNode 挂掉，整个 HDFS 就无法访问，同时 Hadoop 生态系统中依赖于 HDFS 的各个组件，包括 MapReduce、Hive、Pig 以及 HBase 等也都无法正常工作，并且重新启动 NameNode 和进行数据恢复的过程也会比较耗时。\n\n这些问题在给 Hadoop 的使用者带来困扰的同时，也极大地限制了 Hadoop 的使用场景，使得 Hadoop 在很长的时间内仅能用作离线存储和离线计算，无法应用到对可用性和数据一致性要求很高的在线应用场景中。\n\n所幸的是，在 Hadoop2.0 中，HDFS NameNode 和 YARN ResourceManger(JobTracker 在 2.0 中已经被整合到 YARN ResourceManger 之中) 的单点问题都得到了解决，经过多个版本的迭代和发展，目前已经能用于生产环境。\n\n![](https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/img001.png)\n\n从上图中我们可以看到，有两台 NameNode——Active NameNode 和 Standby NameNode，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。\n\n### Yarn\n\n`Yarn` 是 Hadoop 集群的新一代资源管理系统。Hadoop 2.0 对 MapReduce 框架做了彻底的设计重构，我们称 Hadoop 2.0 中的 MapReduce 为 MRv2 或者 Yarn。\n\n![20151029092726524 (2).jpg](https://i.loli.net/2018/12/30/5c28775f7eaa5.jpg)\n\n在 Hadoop 2.x 中，Yarn 把 job 的概念换成了 `application`，因为运行的应用不只是 MapReduce 了，还可能是其他应用，如一个 DAG（有向无环图 Directed Acyclic Graph，例如 Storm 应用）。\n\nYarn 另一个目标是扩展 Hadoop，使得它不仅仅可以支持 MapReduce 计算，还能很方便地管理诸如 Hive、Pig、Hbase、Spark/Shark 等应用。\n\n这种新的架构设计能够使得各种类型的应用运行在 Hadoop 上面，并通过 Yarn 从系统层面进行统一的管理，也就是说，有了 Yarn，**各种应用就可以互不干扰的运行在同一个 Hadoop 系统中**，共享整个集群资源。\n\n\n\n###  ResourceManager 在做些什么\n\n刚刚提到的 Yarn 也采用了 Master/Slave 结构，其中 Master 为 **ResourceManager**，负责整个集群的资源管理与调度；Slave 实现为 **NodeManager**，负责单个节点的组员管理与任务启动。 \n\nResourceManager 是整个 Yarn 集群中最重要的组件之一，它的功能较多，包括 ApplicationMaster 管理（启动、停止等）、NodeManager 管理、Application 管理、状态机管理等。\n\nResourceManager 主要完成以下几个功能：\n- 与客户端交互，处理来自客户端的请求\n- 启动和管理 ApplicationMaster，并在它失败时重新启动它\n- 管理 NodeManager，接受来自 NodeManager 的资源管理汇报信息，并向 NodeManager 下达管理命令或把信息按照一定的策略分配给各个应用程序（ApplicationManager）等\n- **资源管理与调度，接受来自 ApplicationMaster 的资源申请请求，并为之分配资源（核心）**\n\n在 Master/Slave 架构中，ResourceManager 同样存在单点故障（高可用问题，High Availability）问题。为了解决它，通常采用热备方案，即集群中存在一个对外服务的 Active Master 和若干个处于就绪状态的 Standy Master，一旦 Active Master 出现故\n障，立即采用一定的侧率选取某个 Standy Master 转换为 Active Master 以正常对外提供服务。\n\n##  总结\n\n本文介绍了 Hadoop 的相关概念，包括量大核心部件 HDFS 和 MapReduce，并对其进行了进一步剖析，Hadoop 2.0 的 Yarn 的简单介绍，以及一些问题的解决方法（如 HA）。\n\n也通过配置第一次在本机上配置了 Hadoop 的运行环境，运行了伪分布式样例。\n\n接下来会结合一个具体问题深入理解 Hadoop 的方方面面。\n\n<br />\n\n>   References:\n>    [大数据学习笔记](https://chu888chu888.gitbooks.io/hadoopstudy/content/)\n>    [一文读懂大数据平台——写给大数据开发初学者的话!](https://zhuanlan.zhihu.com/p/26545566)\n>   [Hadoop HDFS和MapReduce](https://www.jianshu.com/p/ed6b35f52e3c)\n>   [HDFS文件操作](http://pangjiuzala.github.io/2015/08/03/HDFS%E6%96%87%E4%BB%B6%E6%93%8D%E4%BD%9C/)\n>   [hadoop笔记4--MapReduce框架](https://www.jianshu.com/p/35be7bdca902)\n>   [Hadoop Yarn详解](https://blog.csdn.net/suifeng3051/article/details/49486927)\n>   [Hadoop NameNode 高可用 (High Availability) 实现解析](https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-name-node/index.html)\n> [Hadoop -YARN ResourceManager 剖析](https://blog.csdn.net/zhangzhebjut/article/details/37730065)\n\n<hr />\n","tags":["Hadoop"],"categories":["知识总结"]},{"title":"[总结|展望] 世界不会因为你的无知而停下脚步","url":"%2F2018%2F12%2F10%2F%E6%80%BB%E7%BB%93-%E5%B1%95%E6%9C%9B-%E4%B8%96%E7%95%8C%E4%B8%8D%E4%BC%9A%E5%9B%A0%E4%B8%BA%E4%BD%A0%E7%9A%84%E6%97%A0%E7%9F%A5%E8%80%8C%E5%81%9C%E4%B8%8B%E8%84%9A%E6%AD%A5.html","content":"\n**Be the greatest, or nothing**。\n\n不久前接到这学期期末考试的时间安排表，没想到时间过得这么快。在我所认知的世界里，感到时光飞逝大概有两种原因：强烈热衷于某一事物而忘记时间和玩物丧志在不知不觉中荒废时间，我想，我属于后者。\n\n这学期的我依旧在学习技术、忠于兴趣、力求做到更好，却又懒惰、贪心、自我怀疑、无所适从、对未来没有信心。\n\n回首看来，我竟然如此疯狂，用这种态度对待生命中最宝贵的时光。\n\n\n<!-- more -->\n\n## 期末安排\n\n对于本学期最大的弥补，我所能想到的就是好好准备期末考试，把绩点再冲上一个台阶。\n\n实际上，我是相当讨厌单纯刷绩点这种行为，它大概是初高中应试教育所遗留的糟粕。每个学期，我都会把学习重心放在技术而非学院所开设的课程上，这并非我不重视基础，而是教学安排的确不完全适合我。\n\n尤其是这学期，开了两门无用的语言课（Java、C#），而且教学质量较差，前几天一个水平较高的同学还跟我哭诉现在还敲不出来像样的 Java 代码。余下三门专业课（数字逻辑、离散、汇编）也平淡无奇，甚至有的老师让我想起了 **PPT Reader** 这个词。 \n\n相比之下，我一位在相对较好的学校的同学这学期已经对算法和系统都有了不浅的认识，他们的硬件课教材是 CSAPP，作业也大概是 CMU 15-213 的改进版本。\n\n事实上，现在说刷本学期绩点似乎是个荒唐的事情，毕竟我平时加分少的可怜，甚至旷课被逮到。我能做的，就是尽量在期末的考试中把成绩得到最高，这算是尽力了吧。\n\n##  比赛\n\n大二的我开始接触更多的比赛，不过上学期也仅仅在四科竞赛中得到了 Java 组的一个奖项（还不确定）。是时候为我的简历增些光彩了，所以我报名了蓝桥杯的团队赛和个人赛，并开始了解 PAT。\n\n假期我会以准备比赛和提升英语技能为主，尤其是个人赛，我计划尽力拿到国家级的奖项，团体赛目前不是特别了解，不好立 flag。在经历了四级的失望后，我决心拿出大量时间准备英语，特别是听力方面，假期我会进行一些安排。\n\n包括下学期的几个比赛，大学生英语竞赛、数学建模校赛，我都会着手准备，并力争靠前的名次。\n\n##  项目\n\n讲真，作为走开发路线的我，目前的进展可以算得上很慢了。刚刚把 JavaWeb 的生态熟悉了一遍，可惜院里 Java 的项目太少，也没有得到合适的锻炼机会。不过下学期即便没有好的机会，我也会主动联系老师，真的不能再等了。\n\n##  展望\n\n大二接下来的日子对我大学生涯意义重大，我希望可以通过这段时间证明自己，暂时我不会再好高骛远，而是着手应对当下，我希望在大二结束的那一刻，再翻看这篇文章，能做到问心无愧。\n\n> 要像疯马一样奔跑，快，再快，没有人会等你，弗莱切说得很对，这世上最伤人的句子就是 Good job。\n>   Good job，哦，我做的还不错，我差不多可以了。\n> 不不不！愿没有，你远未愤怒，也远未觉悟，你那些梦想和努力，不过是廉价童话里说说而已。\n> \n> 我们根本就没有努力到与人拼天赋的地位，而我们却像当然的，以为我们的失败只是因为缺乏运气。\n> 我们就是弗莱切嘴里的 Mother fucker，而我们仍然沾沾自喜。\n> 这世上的伟大，世上的成功，哪有一蹴而就。\n> 所谓峰回路转，崖下秘籍，都是故事里哄骗读者的伎俩，而真实的世界，是要见血的。\n> \n> 残酷的励志，励志与鸡汤本来就是两种东西，真实的励志。就是打倒了，爬起来，浑身是血，又聋又瞎，成功的最后，很可能什么也得不到。\n> 很可能，你也将早早死去。\n> 但烈火是你点的，你说要烧一座山，就要做好烧死自己的觉悟。\n> 大火降至。\n> Be the greatest, or nothing。\n> \n> ——朱炫  《爆裂鼓手》影评\n \n<img src=\"https://i.loli.net/2018/12/10/5c0e74b9d6e0b.jpg\" alt=\"\" style=\"width:100%\" />\n\n<hr />\n","tags":["总结"],"categories":["人生苦旅"]},{"title":"就决定是你了 | 为你的终端安装 Pokemon 皮肤","url":"%2F2018%2F11%2F27%2F%E5%B0%B1%E5%86%B3%E5%AE%9A%E6%98%AF%E4%BD%A0%E4%BA%86-%E4%B8%BA%E4%BD%A0%E7%9A%84%E7%BB%88%E7%AB%AF%E5%AE%89%E8%A3%85-Pokemon-%E7%9A%AE%E8%82%A4.html","content":"\n正值精灵宝可梦大热时期，在逛 GitHub 时发现了一个特别强的东西 —— [Pokemon-Terminal](https://github.com/LazoCoder/Pokemon-Terminal)，经过一顿折腾后，终于把终端打造成了这个样子 👇\n\n<img src=\"http://pi0evhi68.bkt.clouddn.com/A5592C04-B48F-47E4-BBB8-3BA763D5F668.png\" alt=\"\" style=\"width:100%\" />\n\n<!-- more -->\n\n##  Pokemon-Terminal\n\n正值精灵宝可梦大热时期，在逛 GitHub 时发现了一个特别强的东西 —— [Pokemon-Terminal](https://github.com/LazoCoder/Pokemon-Terminal)\n\n这是一款美化终端的神器，将口袋妖怪与终端完美结合，先上几张图让大家感受一下：\n\n![Pokemon Terminal README md at master · LazoCoder Pokemon Terminal](http://pi0evhi68.bkt.clouddn.com/Pokemon Terminal README md at master · LazoCoder Pokemon Terminal.png)\n\n\n它拥有 719 款 Pokemon 皮肤，可以根据编号或口袋妖怪名字（例如 pikachu）改变，支持 iTerm2、ConEmu、Terminology、Tilix 等终端，同时支持 Windows、MacOS、GNOME、Openbox 和 i3wm。\n\n如果你也是个口袋迷，那么快来给你的终端安上这款皮肤吧！\n\n##  安装\n\n本项目的 README 上有各种安装方法，这里以 macOS 为例。\n\n首先确保你的电脑已经安装 3.6 及以上版本的 python（最好是 3.6），下面是下载地址\n\n- [For Mac](https://www.python.org/downloads/mac-osx/)\n- [For Windows](https://www.python.org/downloads/windows/)\n- [For Ubuntu](https://askubuntu.com/a/865569)\n- [For Arch Linux](https://www.archlinux.org/packages/extra/x86_64/python/)\n\n确保有以下终端模拟器中的一种（我用的是 iTerm2）\n\n- [iTerm2](https://iterm2.com/)\n- [ConEmu](https://conemu.github.io/) or derivative (such as [Cmder](http://cmder.net/))\n- [Terminology](https://www.enlightenment.org/about-terminology)\n- [Tilix](https://gnunn1.github.io/tilix-web/)\n\n可以使用以下几种方式安装\n\n- [Arch Linux User Repository package (System-wide)](https://aur.archlinux.org/packages/pokemon-terminal-git/) \n- [pip (System-wide)](#pip-system-wide)\n- [pip (Per-User)](#pip-per-user)\n- [npm (Per-User)](#npm-per-user)\n- [Distutils (System-wide)](#distutils-system-wide)\n\n这里我使用 npm 安装（确保有 node.js），因为比较简单。\n\n在 iTerm 2 中输入以下命令\n\n```shell\nnpm install --global pokemon-terminal\n```\n\n好了，这就安装成功了，是不是非常简单！\n\n```bash\n$ pokemon pikachu\n```\n\n皮卡丘，就决定是你了!\n\n##  深度使用\n\n每次启动都想`自动随机`更换皮肤的话，可以像这样设置：\n\n![3806757A-C9B3-41AD-8D70-637CC9DFFF29](http://pi0evhi68.bkt.clouddn.com/3806757A-C9B3-41AD-8D70-637CC9DFFF29.png)\n\n还有原项目给出的使用方法：\n\n```\nusage: pokemon [-h] [-n NAME]\n               [-r [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]]]\n               [-l [0.xx]] [-d [0.xx]]\n               [-t [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]]]\n               [-ne] [-e] [-ss [X]] [-w] [-v] [-dr] [-c]\n               [id]\n\nSet a pokemon to the current terminal background or wallpaper\n\npositional arguments:\n  id                    Specify the wanted pokemon ID or the exact (case\n                        insensitive) name\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -c, --clear           Clears the current pokemon from terminal background\n                        and quits.\n\nFilters:\n  Arguments used to filter the list of pokemons with various conditions that\n  then will be picked\n\n  -n NAME, --name NAME  Filter by pokemon which name contains NAME\n  -r [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]], --region [{kanto,johto,hoenn,sinnoh,unova,kalos} [{kanto,johto,hoenn,sinnoh,unova,kalos} ...]]\n                        Filter the pokemons by region\n  -l [0.xx], --light [0.xx]\n                        Filter out the pokemons darker (lightness threshold\n                        lower) then 0.xx (default is 0.7)\n  -d [0.xx], --dark [0.xx]\n                        Filter out the pokemons lighter (lightness threshold\n                        higher) then 0.xx (default is 0.42)\n  -t [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]], --type [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} [{normal,fire,fighting,water,flying,grass,poison,electric,ground,psychic,rock,ice,bug,dragon,ghost,dark,steel,fairy} ...]]\n                        Filter the pokemons by type.\n  -ne, --no-extras      Excludes extra pokemons (from the extras folder)\n  -e, --extras          Excludes all non-extra pokemons\n\nMisc:\n  -ss [X], --slideshow [X]\n                        Instead of simply choosing a random pokemon from the\n                        filtered list, starts a slideshow (with X minutes of\n                        delay between pokemon) in the background with the\n                        pokemon that matched the filters\n  -w, --wallpaper       Changes the desktop wallpaper instead of the terminal\n                        background\n  -v, --verbose         Enables verbose output\n  -dr, --dry-run        Implies -v and doesn't actually changes either\n                        wallpaper or background after the pokemon has been\n                        chosen\n\nNot setting any filters will get a completely random pokemon\n```\n\n举几个例子，可以根据口袋妖怪的名字改变皮肤\n\n![](https://i.imgur.com/DfA2lcd.gif)\n\n同一款皮肤（部分）还可以改变不同的形态\n\n![](https://i.imgur.com/gdGUucu.gif)\n\n还可以自定义图片之类的，自己摸索吧。\n\n##  终端美化\n\n作者建议更改终端默认的透明度的模糊程度，以达到更好的效果，可以像这样设置：\n\n![](https://i.imgur.com/xSZAGhL.png)\n\n设置之后就会变成这个样子：\n\n![](https://i.imgur.com/82DAT97.jpg)\n\niTerm 2 的默认功能还是不够强大，可以配置 oh-my-zsh，安装字体库、插件等，如果有需要可以参考这篇文章 [iTerm2 + Oh My Zsh 打造舒适终端体验](https://segmentfault.com/a/1190000014992947)。\n\n最后，安装配置了 iTerm 2 + oh-my-zsh + Pokemon-Terminal，你就拥有了像下面一样的终端。\n\n![404B150B-328D-4038-B142-06C0CDDEC40A](http://pi0evhi68.bkt.clouddn.com/404B150B-328D-4038-B142-06C0CDDEC40A.png)\n\nHave fun !\n\n<hr />\n","tags":["美化"],"categories":["折腾"]},{"title":"IoC容器浅析及简单实现","url":"%2F2018%2F11%2F25%2FIoC%E5%AE%B9%E5%99%A8%E6%B5%85%E6%9E%90%E5%8F%8A%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0.html","content":"\nSpring IoC 容器是 Spring 框架中最核心的部分，也是初学者难以理解的部分，对于这种关键的设计，简单实现一次能最大限度地加深理解，了解其中思想，对以后的开发也大有裨益。\n\n\n\n<!-- more -->\n\n#   Spring IoC 容器浅析及简单实现\n\n\n##\tSpring IoC 概述\n\n原生的 JavaEE 技术中各个模块之间的联系较强，即`耦合度较高`。\n\n比如完成一个用户的创建事务，视图层会创建业务逻辑层的对象，再在内部调用对象的方法，各个模块的`独立性很差`，如果某一模块的代码发生改变，其他模块的改动也会很大。\n\n而 Spring 框架的核心——IoC（控制反转）很好的解决了这一问题。控制反转，即`某一接口具体实现类的选择控制权从调用类中移除，转交给第三方决定`，即由 Spring 容器借由 Bean 配置来进行控制。\n\n可能 IoC 不够开门见山，理解起来较为困难。因此， Martin Fowler 提出了 DI（Dependency Injection，依赖注入）的概念来替代 IoC，即`让调用类对某一接口实现类的依赖关系由第三方（容器或写协作类）注入，以移除调用类对某一接口实现类的依赖`。\n\n比如说， 上述例子中，视图层使用业务逻辑层的接口变量，而不需要真正 new 出接口的实现，这样即使接口产生了新的实现或原有实现修改，视图层都能正常运行。\n\n从注入方法上看，IoC 主要划分为三种类型：构造函数注入、属性注入和接口注入。在开发过程中，一般使用`属性注入`的方法。\n\nIoC 不仅可以实现`类之间的解耦`，还能帮助完成`类的初始化与装配工作`，让开发者从这些底层实现类的实例化、依赖关系装配等工作中解脱出出来，专注于更有意义的业务逻辑开发工作。\n\n##\tSpring IoC 简单实现\n\n下面实现了一个IoC容器的核心部分，简单模拟了IoC容器的基本功能。\n\n\n下面列举出核心类：\n\nStudent.java\n\n```java\n/**\n * @ClassName Student\n * @Description 学生实体类\n * @Author Yixiang Zhao\n * @Date 2018/9/22 9:19\n * @Version 1.0\n */\npublic class Student {\n\n    private String name;\n\n    private String gender;\n\n    public void intro() {\n        System.out.println(\"My name is \" + name + \" and I'm \" + gender + \" .\");\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public String getGender() {\n        return gender;\n    }\n\n    public void setGender(String gender) {\n        this.gender = gender;\n    }\n}\n```\n\nStuService.java\n\n```java\n/**\n * @ClassName StuService\n * @Description 学生Service\n * @Author Yixiang Zhao\n * @Date 2018/9/22 9:21\n * @Version 1.0\n */\npublic class StuService {\n\n    private Student student;\n\n    public Student getStudent() {\n        return student;\n    }\n\n    public void setStudent(Student student) {\n        this.student = student;\n    }\n}\n```\n\nbeans.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<beans>\n    <bean id=\"Student\" class=\"me.seriouszyx.pojo.Student\">\n        <property name=\"name\" value=\"ZYX\"/>\n        <property name=\"gender\" value=\"man\"/>\n    </bean>\n\n    <bean id=\"StuService\" class=\"me.seriouszyx.service.StuService\">\n        <property ref=\"Student\"/>\n    </bean>\n</beans>\n```\n\n下面是核心类 ClassPathXMLApplicationContext.java\n\n```java\n\n/**\n * @ClassName ClassPathXMLApplicationContext\n * @Description ApplicationContext的实现，核心类\n * @Author Yixiang Zhao\n * @Date 2018/9/22 9:40\n * @Version 1.0\n */\npublic class ClassPathXMLApplicationContext implements ApplicationContext {\n\n    private Map map = new HashMap();\n\n    public ClassPathXMLApplicationContext(String location) {\n        try {\n            Document document = getDocument(location);\n            XMLParsing(document);\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n    }\n\n    // 加载资源文件，转换成Document类型\n    private Document getDocument(String location) throws JDOMException, IOException {\n        SAXBuilder saxBuilder = new SAXBuilder();\n        return saxBuilder.build(this.getClass().getClassLoader().getResource(location));\n    }\n\n    private void XMLParsing(Document document) throws Exception {\n        // 获取XML文件根元素beans\n        Element beans = document.getRootElement();\n        // 获取beans下的bean集合\n        List beanList = beans.getChildren(\"bean\");\n        // 遍历beans集合\n        for (Iterator iter = beanList.iterator(); iter.hasNext(); ) {\n            Element bean = (Element) iter.next();\n            // 获取bean的属性id和class，id为类的key值，class为类的路径\n            String id = bean.getAttributeValue(\"id\");\n            String className = bean.getAttributeValue(\"class\");\n            // 动态加载该bean代表的类\n            Object obj = Class.forName(className).newInstance();\n            // 获得该类的所有方法\n            Method[] methods = obj.getClass().getDeclaredMethods();\n            // 获取该节点的所有子节点，子节点存储类的初始化参数\n            List<Element> properties = bean.getChildren(\"property\");\n            // 遍历，将初始化参数和类的方法对应，进行类的初始化\n            for (Element pro : properties) {\n                for (int i = 0; i < methods.length; i++) {\n                    String methodName = methods[i].getName();\n                    if (methodName.startsWith(\"set\")) {\n                        String classProperty = methodName.substring(3, methodName.length()).toLowerCase();\n                        if (pro.getAttribute(\"name\") != null) {\n                            if (classProperty.equals(pro.getAttribute(\"name\").getValue())) {\n                                methods[i].invoke(obj, pro.getAttribute(\"value\").getValue());\n                            }\n                        } else {\n                            methods[i].invoke(obj, map.get(pro.getAttribute(\"ref\").getValue()));\n                        }\n                    }\n                }\n            }\n            // 将初始化完成的对象添加到HashMap中\n            map.put(id, obj);\n        }\n    }\n\n    public Object getBean(String name) {\n        return map.get(name);\n    }\n\n}\n```\n\n最后进行测试\n\n```java\npublic class MyIoCTest {\n    public static void main(String[] args) {\n        ApplicationContext context = new ClassPathXMLApplicationContext(\"beans.xml\");\n        StuService stuService = (StuService) context.getBean(\"StuService\");\n        stuService.getStudent().intro();\n    }\n}\n```\n\n测试成功！\n\n```text\nMy name is ZYX and I'm man .\n\nProcess finished with exit code 0\n```\n\n##\t源码\n\n代码在我的 [GitHub](https://github.com/seriouszyx/LearnSpring/tree/master/mycode/SimpleIoC)开源，欢迎一起交流讨论。\n\n##\t总结\n\n熟悉一个框架最好的方式，就是亲手实现它。这样不仅会深刻地认识到框架的工作原理，以后的使用也会更加得心应手。\n\n此外，在实现的过程中，又会收获很多东西，就像实现 IoC 容器一样，不仅了解解析 XML 文件的 JDOM 工具，还加深了对 Java 反射的理解。在实际开发中，几乎没有任何地方需要用到反射这一技术，但在框架实现过程中，不懂反射则寸步难行。\n\n>\t更多的 Spring 学习心得请戳[Spring 框架学习](https://github.com/seriouszyx/LearnSpring)","tags":["Java"],"categories":["知识总结"]}]